#!/usr/bin/env python3
"""
3D Dataset Image Downloader
ä»Pexels APIä¸‹è½½3Dæ•°æ®é›†é¢„è§ˆç…§ç‰‡ï¼Œç‰¹åˆ«å…³æ³¨Material Textureså’ŒLighting Scenarios
"""

import requests
import os
import json
import time
from pathlib import Path
from typing import List, Dict, Optional
import argparse

class PexelsImageDownloader:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.pexels.com/v1/search"
        self.headers = {
            "Authorization": api_key
        }
        
        # åˆ›å»ºä¸‹è½½ç›®å½•
        self.download_dir = Path("public/images/datasets")
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        # 3Dæ•°æ®é›†ç›¸å…³çš„æœç´¢æŸ¥è¯¢
        self.search_queries = {
            "material_textures": [
                "fabric texture",
                "metal texture", 
                "wood texture",
                "stone texture",
                "leather texture",
                "glass material",
                "ceramic texture",
                "plastic material",
                "rubber texture",
                "marble texture",
                "brick texture",
                "concrete texture",
                "fabric close up",
                "metal surface",
                "wood grain",
                "stone surface"
            ],
            "lighting_scenarios": [
                "studio lighting",
                "natural lighting",
                "golden hour",
                "blue hour",
                "soft lighting",
                "hard lighting",
                "dramatic lighting",
                "ambient lighting",
                "backlighting",
                "side lighting",
                "top lighting",
                "rim lighting",
                "volumetric lighting",
                "mood lighting",
                "product lighting",
                "portrait lighting"
            ],
            "3d_objects": [
                "3d model",
                "product photography",
                "object photography",
                "still life",
                "geometric shapes",
                "abstract objects",
                "minimalist objects",
                "modern objects",
                "design objects",
                "architectural elements"
            ]
        }
    
    def search_images(self, query: str, per_page: int = 10) -> Optional[Dict]:
        """æœç´¢å›¾ç‰‡"""
        params = {
            "query": query,
            "per_page": per_page,
            "orientation": "square"  # æ­£æ–¹å½¢å›¾ç‰‡æ›´é€‚åˆ3Dé¢„è§ˆ
        }
        
        try:
            response = requests.get(self.base_url, headers=self.headers, params=params)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"æœç´¢å¤±è´¥ '{query}': {e}")
            return None
    
    def download_image(self, url: str, filename: str) -> bool:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            file_path = self.download_dir / filename
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {filename}")
            return True
        except requests.exceptions.RequestException as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥ '{filename}': {e}")
            return False
    
    def download_category_images(self, category: str, queries: List[str], images_per_query: int = 3) -> Dict[str, List[str]]:
        """ä¸‹è½½ç‰¹å®šç±»åˆ«çš„å›¾ç‰‡"""
        print(f"\nğŸ¯ å¼€å§‹ä¸‹è½½ {category} ç±»åˆ«å›¾ç‰‡...")
        
        downloaded_files = []
        category_dir = self.download_dir / category
        category_dir.mkdir(exist_ok=True)
        
        for i, query in enumerate(queries, 1):
            print(f"\nğŸ“¸ æœç´¢: '{query}' ({i}/{len(queries)})")
            
            # æœç´¢å›¾ç‰‡
            search_result = self.search_images(query, per_page=images_per_query)
            if not search_result or not search_result.get('photos'):
                print(f"âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡: {query}")
                continue
            
            # ä¸‹è½½å›¾ç‰‡
            for j, photo in enumerate(search_result['photos'][:images_per_query]):
                # ä½¿ç”¨mediumå°ºå¯¸ï¼Œé€‚åˆé¢„è§ˆ
                image_url = photo['src']['medium']
                
                # ç”Ÿæˆæ–‡ä»¶å
                filename = f"{category}_{i:02d}_{j+1:02d}_{photo['id']}.jpg"
                file_path = category_dir / filename
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if file_path.exists():
                    print(f"â­ï¸  æ–‡ä»¶å·²å­˜åœ¨: {filename}")
                    downloaded_files.append(str(file_path.relative_to(self.download_dir)))
                    continue
                
                # ä¸‹è½½å›¾ç‰‡
                if self.download_image(image_url, str(file_path.relative_to(self.download_dir))):
                    downloaded_files.append(str(file_path.relative_to(self.download_dir)))
                
                # é¿å…APIé™åˆ¶
                time.sleep(0.5)
        
        return {category: downloaded_files}
    
    def download_all_categories(self, images_per_query: int = 3) -> Dict[str, List[str]]:
        """ä¸‹è½½æ‰€æœ‰ç±»åˆ«çš„å›¾ç‰‡"""
        print("ğŸš€ å¼€å§‹ä¸‹è½½3Dæ•°æ®é›†é¢„è§ˆå›¾ç‰‡...")
        print(f"ğŸ“ ä¸‹è½½ç›®å½•: {self.download_dir}")
        
        all_results = {}
        
        for category, queries in self.search_queries.items():
            results = self.download_category_images(category, queries, images_per_query)
            all_results.update(results)
            
            # ç±»åˆ«é—´æš‚åœï¼Œé¿å…APIé™åˆ¶
            time.sleep(2)
        
        return all_results
    
    def generate_dataset_info(self, results: Dict[str, List[str]]) -> None:
        """ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶"""
        info_file = self.download_dir / "dataset_info.json"
        
        dataset_info = {
            "total_images": sum(len(files) for files in results.values()),
            "categories": {},
            "download_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "description": "3D Dataset Preview Images from Pexels API",
            "categories_description": {
                "material_textures": "å„ç§æè´¨çº¹ç†å›¾ç‰‡ï¼Œç”¨äº3Dæè´¨æ¸²æŸ“å‚è€ƒ",
                "lighting_scenarios": "ä¸åŒå…‰ç…§åœºæ™¯å›¾ç‰‡ï¼Œç”¨äº3Då…‰ç…§æ•ˆæœå‚è€ƒ", 
                "3d_objects": "3Då¯¹è±¡å’Œäº§å“æ‘„å½±ï¼Œç”¨äº3Då»ºæ¨¡å‚è€ƒ"
            }
        }
        
        for category, files in results.items():
            dataset_info["categories"][category] = {
                "count": len(files),
                "files": files,
                "description": dataset_info["categories_description"].get(category, "")
            }
        
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“‹ æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜: {info_file}")
    
    def print_summary(self, results: Dict[str, List[str]]) -> None:
        """æ‰“å°ä¸‹è½½æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š ä¸‹è½½æ‘˜è¦")
        print("="*60)
        
        total_images = 0
        for category, files in results.items():
            count = len(files)
            total_images += count
            print(f"ğŸ“ {category}: {count} å¼ å›¾ç‰‡")
        
        print(f"\nğŸ‰ æ€»è®¡ä¸‹è½½: {total_images} å¼ å›¾ç‰‡")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {self.download_dir}")
        print("="*60)

def main():
    parser = argparse.ArgumentParser(description="ä»Pexels APIä¸‹è½½3Dæ•°æ®é›†é¢„è§ˆå›¾ç‰‡")
    parser.add_argument("--api-key", required=True, help="Pexels APIå¯†é’¥")
    parser.add_argument("--images-per-query", type=int, default=3, help="æ¯ä¸ªæŸ¥è¯¢ä¸‹è½½çš„å›¾ç‰‡æ•°é‡")
    parser.add_argument("--category", choices=["material_textures", "lighting_scenarios", "3d_objects", "all"], 
                       default="all", help="è¦ä¸‹è½½çš„å›¾ç‰‡ç±»åˆ«")
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = PexelsImageDownloader(args.api_key)
    
    if args.category == "all":
        # ä¸‹è½½æ‰€æœ‰ç±»åˆ«
        results = downloader.download_all_categories(args.images_per_query)
    else:
        # ä¸‹è½½ç‰¹å®šç±»åˆ«
        queries = downloader.search_queries[args.category]
        results = downloader.download_category_images(args.category, queries, args.images_per_query)
    
    # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯
    downloader.generate_dataset_info(results)
    
    # æ‰“å°æ‘˜è¦
    downloader.print_summary(results)

if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œä½¿ç”¨é»˜è®¤APIå¯†é’¥
    API_KEY = "yi0EZidm8kZmEEvn4H643YtHX2BO05YD8yPeHTsnfbMZQEY6RkW7H0wO"
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = PexelsImageDownloader(API_KEY)
    
    # ä¸‹è½½æ‰€æœ‰ç±»åˆ«
    results = downloader.download_all_categories(images_per_query=3)
    
    # ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯
    downloader.generate_dataset_info(results)
    
    # æ‰“å°æ‘˜è¦
    downloader.print_summary(results)
